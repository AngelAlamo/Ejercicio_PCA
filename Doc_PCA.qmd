---
title: "Ejercicio PCA"
author: 
  - name: Ángel Álamo
  - name: Juanjo Doblas
  - name: Óscar Vanrell 
format: html
editor: visual
execute:
  echo: false
---

```{r librerias, echo = FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(factoextra)
library(ggfortify)
library(gridExtra)
library(tidyverse)
```

Primero, vamos a cargar los datos del fichero `europa.dat` y consideraremos los datos centrados, es decir, restamos a cada valor la media de la columna correspondiente.

```{r cargar datos}
datos = read.table("europa.dat")

# Número de filas
n = nrow(datos)

# Matriz de centrado
Hn = diag(n) - 1/n

# Datos centrados
datos_cen = Hn %*% as.matrix(datos)

rownames(datos_cen) <- rownames(datos)
```

Antes de reducir el número de variables a partir del método de componentes principales, observemos la matriz de varianzas-covarianzas y la matriz de correlación:

```{r matriz covarianzas}
S = cov(datos_cen)
print("Matriz de covarianzas")
```

```{r imprimir matriz cov}
S
```

Como los datos corresponden a los porcentajes de población, y por tanto, todas las variables están en una escala común y las diferencias entre las varianzas de las variables son informativas, el estudio utilizando la matriz de covarianzas sin tipificar es correcto. Si miramos la matriz de varianzas-covarianzas, vemos que hay un rango muy alto de variablidad, por ejemplo la variable "Agr" presenta una variación de `r S[1,1]`, en cambio, variables como "Min" o "Ene", presentan una variación de `r S[2,2]` y `r S[4,4]`, respectivamente, que son muy inferiores. También hay variables con una varianza no tan extrema pero muy pequeña respecto "Agr", como es el caso de la variable "Fab", con varianza `r S[3,3]` y "SSP", con varianza `r S[8,8]`.

Por otro lado, la matriz de correlaciones es:

```{r matriz correlaciones}
R = cor(datos_cen)
print("Matriz de correlaciones")
```

```{r imprimir matriz cor}
R
```

En cuanto a las correlaciones, la variable "Agr" es la que presenta más correlación con las otras variables: tenemos una correlacción considerable en cada variable (excepto con "Min"), además, todas estas asociaciones son negativas, las más destacables son `r R[1,3]` con "Fab"; `r R[1,6]` con "Fin" y `r R[1,7]` con "SSP". Por otro lado, respecto a las correlaciones con asociación positiva, tenemos "IS" - "SSP" con una correlación de `r R[6, 8]` y "SSP" - "TC" con `r R[8,9]`.

Una vez visto las matrices, pasamos al estudio de las componentes principales. Siguiendo el método visto en teoría, calculamos los valores y vectores propios de la matriz de varianzas-covarianzas.

```{r PCA}
datos_pca = prcomp(datos_cen, scale = FALSE)
```

```{r veps}
veps = datos_pca$rotation
veps
```

La primera componente principal le da peso mayoritariamente a la variable "Agr", esto es debido a que su variación es muy superior a las demás y esto afecta considerablemente en las componentes principales. Por otro lado, también le da un cierto peso (aunque con signo contrario) a las variables "Fab" y "SSP".

La segunda componente principal da un peso (positivo) muy diferenciado a la variable "Fab". También, le da un peso (con signo contrario) a la variable "SSP", que en principio ya estaban explicadas por la primera componente.

Por último, las otras componentes intentan explicar las demás variables que han quedado fuera de estas dos componentes principales o igualmente dando peso a las variables con más varianza, como PC3 dando aún un peso a "SSP", esto proviene de la gran diferencia que presentan las varianzas de nuestras variables, provocando que aquellas con varianza menor no queden representadas y por lo tanto, el estudio, aunque no incorrecto, no es el adecuado.

```{r vaps}
vaps = get_eigenvalue(datos_pca)
vaps
```

A partir de esta tabla de valores podemos ver que con solamente dos componentes principales nos permiten explicar aproximadamente el $93\%$ de la variación total. Para ver de una manera más detallada el peso de estas componentes, podemos hacer el siguiente gráfico, donde se muestra un diagrama de barras donde a cada dimensión le asociamos el porcentaje de variación que explica:

```{r porcentajes variacion}
fviz_eig(datos_pca, addlabels = TRUE, ylim = c(0,100))
```

Como vemos, a partir de la segunda componente no obtenemos un porcentaje necesario, es decir, el porcentaje de variablidad no aumenta lo suficiente como para considerar otra componente principal. Con esto concluimos que es de más utilidad perder variabilidad pero una mayor facilidad de respresentación con una dimensión menor. Representemos estas variables en el espacio:

```{r circulo correlacion dim1-dim2}
circ_cor <- fviz_pca_var(datos_pca, col.var = "contrib", repel = TRUE, axes = 1:2)

circ_cor
```
En este gráfico de círculo de correlación podemos ver que la variable mejor representada es "Agr" ya que es la que tiene la flecha más alejada del origen, seguida por "Fab" y "SSP"; y la peor representada es "Ene" por tener la flecha más corta que, de hecho, apenas se puede apreciar.  Además, el color de los vectores nos da información sobre la contribución de las variables a este plano principal. Podemos ver claramente que "Agr" es la que más contribuye por tener un color más claro y que las otras apenas contribuyen. En efecto, si miramos las contribuciones de cada variable al plano:

```{r contribuciones plano principal}
# Media ponderada de las contribuciones respecto al porcentaje de variación
t(get_pca_var(datos_pca)$contrib[, 1:2] %*% vaps$variance.percent[1:2])/(sum(vaps$variance.percent[1:2]))
```

notamos que la variable "Agr" contribuye un $70\%$ al plano, mientras que las otras no superan el $14\%$. También podemos ver de una forma más clara que la variable que menos contribuye es "Ene".

Veamos también la calidad de representación de las variables con un gráfico de barras cuyas alturas nos dan el valor `cos2` de las variables:

```{r grafico barras cos2}
fviz_cos2(datos_pca, choice = "var", axes = 1:2)
```

Con este gráfico podemos ver la gran diferencia de calidad de representación que hay entre la variable "Agr" y todas las demás, siendo "Fab" y "SSP" las que le siguen y "Ene" la peor representada.

```{r Interpretacion Angel del circulo de correlacion}
#Con esta representación gráfica, observamos que la contribución de "Agr" es demasiado alta, esto hace que las componentes principales estén afectadas por esta diferencia, haciendo que las otras variables no tengan una calidad de representación correcta, esto nos lleva a realizar el estudio con las variables tipificadas.

#Por último, antes del siguiente análisis, veamos si a partir de estas componentes principales hemos podido obtener algunas agrupaciones de países.
```

Por último, veamos si a partir de estas componentes principales hemos podido obtener algunas agrupaciones de países.

```{r representacion individuos, fig.width=12, fig.height=4}
rep_ind <- fviz_pca_ind(datos_pca, col.ind = "cos2", repel = TRUE, axes = 1:2,
                           geom = "point"
                           ) 

grid.arrange(rep_ind, circ_cor, ncol = 2)

#autoplot(datos_pca, data = datos_cen, loadings = TRUE, loadings.colour = 'blue', loadings.label = TRUE, loadings.label.size = 3, label = TRUE)
```

No se puede ver de manera clara agrupaciones entre los países ya que estos parecen seguir una dispersión "uniforme". Nuestra decisión consiste en hacer tres grupos: los países que se encuentran en el segundo cuadrante, los del tercer cuadrante y los que se encuentran a la derecha del eje vertical. Esta agrupación sigue aproximadamente la estructura dada por las variables cuyo vector en el plano es más largo.

La tabla de datos respecto a las componentes principales es la siguiente:


```{r}
tabla_pca = datos_pca$x[,1:2]
names = datos[,0]
tabla_pca = cbind(names, tabla_pca)

tabla_pca
```
Estudiaremos los grupos principalmente sobre la variable "Agr", ya que esta presenta con gran diferencia una varianza superior.

Respecto al primer grupo, podemos decir que son países en los que el porcentaje de población que se dedica a la agricultura y a los servicios sociales y personales está por debajo de la media grupal, mientras que el porcentaje dedicado al sector fábricas es superior a la media.

```{r}
grupo1 = tabla_pca[tabla_pca$PC1 <= 0 & tabla_pca$PC2 >= 0,]
grupo1

```

Si vemos los valores que toma la variable "Agr", SSP" y "Fab" en estos países:

```{r}
names_gr1 = rownames(grupo1)

tabla_grupo1 = datos %>%
  filter(rownames(.) %in% names_gr1)

tabla_grupo1

```

En la variable "Agr": presentan una media muestral `r mean(tabla_grupo1$Agr) `, en cambio, la media muestral global es `r mean(datos$Agr)`, bastante superior,

En la variable "SSP": presentan una media muestral `r mean(tabla_grupo1$SSP) `,por otro lado, la media muestral global es `r mean(datos$SSP)`, que no se notan grandes diferencias aunque es superior.

En la variable "Fab": la media muestral de este grupo es `r mean(tabla_grupo1$Fab) `, si consideramos todos los datos, la media muestral que obtenemos es `r mean(datos$Fab)`, considerablemente menor, como dijimos anteriormente.



Para el segundo grupo, también son paises que se dedican menos a la agricultura. Pero al contrario que en el primer grupo, son países cuya actividad destaca más en el sector servicios sociales y personales y menos en el sector fábricas.


```{r}
grupo2 = tabla_pca[tabla_pca$PC1 <= 0 & tabla_pca$PC2 <= 0,]
grupo2
```


```{r}
names_gr2 = rownames(grupo2)

tabla_grupo2 = datos %>%
  filter(rownames(.) %in% names_gr2)

tabla_grupo2

```

La media muestral en agricultura de este grupo es de `r mean(tabla_grupo2$Agr)`, de aquí deducimos que este grupo presenta menos dedicación a la agricultura, respecto al grupo anterior (con poca diferencia) y notablemente respecto el total.

Si miramos la variable de Servicios Sociales y Personales: la media muestral es `r mean(tabla_grupo2$SSP) `, superior a las medias muestrales del grupo1 y del global, vistas anteriormente.

Respecto el sector de fábricas, presentan una media muestral de `r mean(tabla_grupo2$Fab) `, inferior a la del grupo1 y aproximada al global.


Por último, el tercer grupo son países que se dedican especialmente a la agricultura, con un porcentaje superior a la media. Destacar el caso de Turquía, que se puede apreciar claramente que es el país que más porcentaje de población dedica a este sector.

```{r}
grupo3 = tabla_pca[tabla_pca$PC1 >= 0,]
grupo3
```
```{r}
names_gr3 = rownames(grupo3)

tabla_grupo3 = datos %>%
  filter(rownames(.) %in% names_gr3)

tabla_grupo3

```
Como vemos, Turquia destaca claramente en el sector de agricultura, podríamos considerarlo como un $\textit{outlayer}$. Respecto a la variable agricultura: su mmedia muestral es `r mean(tabla_grupo3$Agr)`, bastante superior respecto a todos los grupos y a la media muestral total, llegando casi a duplicarla.


Por último, podemos hacer un boxplot de cada grupo respecto la variable "Agr" para ver la diferencias que presentan:


```{r}

datos = datos %>% 
  mutate("Grupo" = case_when(
    rownames(.) %in% names_gr1 ~ "Grupo1",
    rownames(.) %in% names_gr2 ~ "Grupo2",
    rownames(.) %in% names_gr3 ~ "Grupo3",
    )) %>% 
  relocate(Grupo, .before = Agr)



boxplot1 = datos %>% 
  ggplot() + 
  geom_boxplot(aes(x = Grupo, y = Agr, color = Grupo)) +
  stat_summary(aes(x = Grupo, y = Agr),
               fun = mean, geom = "point", shape = 19, size = 1, color = "black") +
  labs(x = "Grupo", y = "Porcentaje en agricultura", 
       color = "Grupo") +
  theme_bw() + 
  theme(axis.text = element_text(face = "bold"),
        axis.title = element_text(face = "bold"),
        legend.title = element_text(size = 10, face = "bold")) + 
  theme(plot.title=element_text(size=20, face='bold', color="#83CC3A"))
  

boxplot1

```
Claramente el grupo 3 presenta grandes diferencias en este sector.


Respecto al resto de variables, podemos decir que como no están bien representadas en el plano principal, que agrupa el $93\%$ de la variación de los datos, quiere decir que estas varían muy poco en comparación a las tres variables comparadas, y por lo tanto, en general los países se agrupan en unos ciertos porcentajes.



Para ver el código del estudio, visite [este repositorio de GitHub.](https://github.com/AngelAlamo/Ejercicio_PCA)


